{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Two Sentence Horror Bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOOHx6e09W9fQRwXjvjxaKs",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7fca2f98562141ecb7dbabfad7b12179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fcd5cb589aa648a9a9ffd895d64aaab1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce70fe14e7e84150832f7b17fe452bac",
              "IPY_MODEL_740dd6f27c474793b7aabadb2bcc60aa"
            ]
          }
        },
        "fcd5cb589aa648a9a9ffd895d64aaab1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "ce70fe14e7e84150832f7b17fe452bac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7a346b3d730f4f048611a31c909736e3",
            "_dom_classes": [],
            "description": "Validation sanity check:   0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "danger",
            "max": 2,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4243663306d94fc98f8663ea0c440667"
          }
        },
        "740dd6f27c474793b7aabadb2bcc60aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_785fa26a95ab40b9a8d581726841e328",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/2 [00:39&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_245cb62b84924221af208bca2024931f"
          }
        },
        "7a346b3d730f4f048611a31c909736e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4243663306d94fc98f8663ea0c440667": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "785fa26a95ab40b9a8d581726841e328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "245cb62b84924221af208bca2024931f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "429b67d94a3645b5a4905e4039e1d605": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fb33aa5e5f1e4f648f48160fb705e9e4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ee895919b9164b31bc1a250ca72b841a",
              "IPY_MODEL_c86ca5c4531241deab2cdaf953953efe"
            ]
          }
        },
        "fb33aa5e5f1e4f648f48160fb705e9e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "ee895919b9164b31bc1a250ca72b841a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_246f557df7584fb9a8a7a4ecbd01f4b9",
            "_dom_classes": [],
            "description": "Epoch 0: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 50,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7bc319d997ed4f2b893e4b902a28f564"
          }
        },
        "c86ca5c4531241deab2cdaf953953efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_45e11f5dad9e45b397775b767f7aa23b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 50/50 [00:24&lt;00:00,  2.05it/s, loss=3.72, v_num=7]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1c49a3d2f1704e6f909f3a640f5e46fd"
          }
        },
        "246f557df7584fb9a8a7a4ecbd01f4b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7bc319d997ed4f2b893e4b902a28f564": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "45e11f5dad9e45b397775b767f7aa23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1c49a3d2f1704e6f909f3a640f5e46fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "342bb3376e394a4ab068f4f632044d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_bb2485748e4e4e8e8bf3a4f2de96ad3b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_48f23795527c4641af1d4282eec067ac",
              "IPY_MODEL_61b381409f524441ac297797bd4b027b"
            ]
          }
        },
        "bb2485748e4e4e8e8bf3a4f2de96ad3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "48f23795527c4641af1d4282eec067ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e4e7efda3f2e4dbdabb2b3576df9128c",
            "_dom_classes": [],
            "description": "Validating: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 13,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 13,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7b090cd169f94ffa92ea1ad24172e0e0"
          }
        },
        "61b381409f524441ac297797bd4b027b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_588451312e27435abb7bea87ebb48313",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 13/13 [00:00&lt;00:00, 18.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bc41218502dc4dee9cde186bd48b852b"
          }
        },
        "e4e7efda3f2e4dbdabb2b3576df9128c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7b090cd169f94ffa92ea1ad24172e0e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "588451312e27435abb7bea87ebb48313": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bc41218502dc4dee9cde186bd48b852b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sfeucht/bart_horror/blob/main/Two_Sentence_Horror_Bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYrBvIWodW2I"
      },
      "source": [
        "# Two Sentence Horror Bot (@BartHorror)\n",
        "\n",
        "This file fine-tunes a pre-trained BART language model on a million top posts from [r/TwoSentenceHorror](https://www.reddit.com/r/TwoSentenceHorror/). It then samples hot [r/Showerthoughts](https://www.reddit.com/r/Showerthoughts/) and [r/OffMyChest](https://www.reddit.com/r/OffMyChest/) posts to feed into the fine-tuned model as prompts, generating some (hopefully) spooky results. \n",
        "\n",
        "# Examples\n",
        "\n",
        "***See [@BartHorror on Twitter](https://twitter.com/BartHorror) for more examples of this model in action!***\n",
        "\n",
        "I have friends, and I believe most of these friends actually like me back.\n",
        "\n",
        "But I donâ€™t believe theyâ€™ll ever come back.\n",
        "\n",
        "-\n",
        "\n",
        "I'm so happy for me.\n",
        "\n",
        "But now Iâ€™m not so happy for me.\n",
        "\n",
        "-\n",
        "\n",
        "Whoever coined the saying, \"Money can't buy happiness\" never had to buy anti-depressants.\n",
        "\n",
        "If only I'd had the money.\n",
        "\n",
        "-\n",
        "\n",
        "Mount Everest really is the most expensive cemetery.\n",
        "\n",
        "Itâ€™s the most beautiful place in the world.\n",
        "\n",
        "-\n",
        "\n",
        "I canâ€™t tell him this obviously but I totally wouldnâ€™t mind spending the rest of my life with him.\n",
        "\n",
        "I donâ€™t know if Iâ€™ll ever see him again.\n",
        "\n",
        "-\n",
        "\n",
        "I was working in the lab late one night.\n",
        "\n",
        "I'm still working in the lab late at night.\n",
        "\n",
        "-\n",
        "\n",
        "Wood will probably be considered a luxury building material, like marble, when we colonise other star systems. \n",
        "\n",
        "Itâ€™s a shame that we canâ€™t be considered a luxury building material, like marble, when we colonise other human systems.\n",
        "\n",
        "-\n",
        "\n",
        "Dig up someone who died yesterday and you're a criminal, dig up someone who died 1000 years ago and you're an archeologist.\n",
        "\n",
        "As I dig up the remains of someone who died 1000 years ago, I realize that I am not an archeologist.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSrio5-SNeCR"
      },
      "source": [
        "#Setup (imports, installation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsbCuatktRkL",
        "outputId": "2bb844b3-7f78-466b-b847-eb17fc73a10f"
      },
      "source": [
        "!pip install -q pytorch-lightning\n",
        "!pip install -q transformers\n",
        "!pip install praw"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in /usr/local/lib/python3.7/dist-packages (7.2.0)\n",
            "Requirement already satisfied: update-checker>=0.18 in /usr/local/lib/python3.7/dist-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: prawcore<3,>=2 in /usr/local/lib/python3.7/dist-packages (from praw) (2.0.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from praw) (0.58.0)\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from update-checker>=0.18->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.3.0->update-checker>=0.18->praw) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORx8g8CvlfKP"
      },
      "source": [
        "from datetime import datetime\n",
        "import json\n",
        "import random\n",
        "import math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "import praw\n",
        "import argparse\n",
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SfzY5upniEC"
      },
      "source": [
        "#First, collect and preprocess fine-tuning r/TwoSentenceHorror data from Reddit\n",
        "\n",
        "Use the [Python Reddit API Wrapper](https://praw.readthedocs.io/en/latest/getting_started/quick_start.html#read-only) to quickly extract training data from r/TwoSentenceHorror, getting the top million posts of all time. Can easily change the number of top posts we want to train on with `number_top_posts` variable. r/TwoSentenceHorror shouldn't have any empty selftext fields, but if you want to use this code for other subreddits make sure to include the line checking whether `submission.selftext` is an empty string to filter out images, links, and posts with only a title.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ViC-rHhnhSQ"
      },
      "source": [
        "raw_posts = pd.DataFrame(columns=['source', 'target'])\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"client-id\",\n",
        "    client_secret=\"client-secret\",\n",
        "    user_agent=\"bart_horror by sfeucht\",\n",
        "    check_for_async=False\n",
        ")\n",
        "\n",
        "number_top_posts = 1000000\n",
        "for submission, rank in zip(reddit.subreddit('twosentencehorror').top(limit=number_top_posts), range(number_top_posts)):\n",
        "  if submission.selftext != '' and 'advertisement' not in submission.selftext:\n",
        "    source = re.sub('\\*', '', submission.title)\n",
        "    target = re.sub('\\*', '', submission.selftext)\n",
        "    raw_posts.loc[rank] = [source, target]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cEo1CJ_MhgF"
      },
      "source": [
        "#Pytorch Lightning Model Setup\n",
        "\n",
        "Most of this code is taken directly from [this great tutorial](https://towardsdatascience.com/teaching-bart-to-rap-fine-tuning-hugging-faces-bart-model-41749d38f3ef) on fine-tuning BART by Neil Sinclair, as well as the [Pytorch Lightning Docs](https://pytorch-lightning.readthedocs.io/en/latest/starter/rapid_prototyping_templates.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aergjZerNlsk"
      },
      "source": [
        "# Function that takes a model as input (or part of a model) and freezes the layers for faster training, adapted from finetune.py\n",
        "def freeze_params(model):\n",
        "  for layer in model.parameters():\n",
        "    layer.requires_grade = False\n",
        "\n",
        "# Pytorch Lightning model module to hold the BART model \n",
        "class LitModel(pl.LightningModule):\n",
        "    def __init__(self, learning_rate, tokenizer, model, hparams):\n",
        "        super().__init__()\n",
        "        self.learning_rate = learning_rate\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.hparams = hparams\n",
        "\n",
        "        # freeze the positional embedding parameters and encoder for faster training \n",
        "        self.freeze_embeds()\n",
        "        freeze_params(self.model.get_encoder())\n",
        "\n",
        "    # freeze the positional embedding parameters of the model; adapted from finetune.py\n",
        "    def freeze_embeds(self):\n",
        "      freeze_params(self.model.model.shared)\n",
        "      for d in [self.model.model.encoder, self.model.model.decoder]:\n",
        "        freeze_params(d.embed_positions)\n",
        "        freeze_params(d.embed_tokens)\n",
        "    \n",
        "    # Do a forward pass through the model\n",
        "    def forward(self, input_ids, **kwargs):\n",
        "      return self.model(input_ids, **kwargs)\n",
        "\n",
        "    # Boilerplate from Pytorch Lightning rapid prototype templates, w/ custom learning_rate\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
        "        return optimizer\n",
        "\n",
        "    # Train with the titles of posts as source and selftext as target\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        # Load the data into variables\n",
        "        src_ids, src_mask, tgt_ids = batch[0], batch[1], batch[2]\n",
        "\n",
        "        # Shift the decoder tokens right (but NOT the tgt_ids)\n",
        "        decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
        "\n",
        "        # Run the model and get the logits\n",
        "        outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
        "        lm_logits = outputs[0]\n",
        "        # Create the loss function, then calculate the loss on the un-shifted tokens\n",
        "        ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
        "        loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
        "\n",
        "        return {'loss':loss}\n",
        "\n",
        "    # To validate, do the exact same thing\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        # Load the data into variables\n",
        "        src_ids, src_mask, tgt_ids = batch[0], batch[1], batch[2]\n",
        "\n",
        "        # Shift the decoder tokens right (but NOT the tgt_ids)\n",
        "        decoder_input_ids = shift_tokens_right(tgt_ids, tokenizer.pad_token_id)\n",
        "        \n",
        "        # Run the model and get the logits\n",
        "        outputs = self(src_ids, attention_mask=src_mask, decoder_input_ids=decoder_input_ids, use_cache=False)\n",
        "        lm_logits = outputs[0]\n",
        "        # Create the loss function, then calculate the loss on the un-shifted tokens\n",
        "        ce_loss_fct = torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n",
        "        val_loss = ce_loss_fct(lm_logits.view(-1, lm_logits.shape[-1]), tgt_ids.view(-1))\n",
        "\n",
        "        return {'loss': val_loss}\n",
        "\n",
        "    # Function to generate text from the trained model. Generate and then decode all the text generated\n",
        "    def generate_text(self, text, eval_beams, early_stopping = True, max_len = 40):\n",
        "      generated_ids = self.model.generate(\n",
        "          text[\"input_ids\"],\n",
        "          attention_mask=text[\"attention_mask\"],\n",
        "          use_cache=True,\n",
        "          decoder_start_token_id = self.tokenizer.pad_token_id,\n",
        "          num_beams = eval_beams,\n",
        "          max_length = max_len,\n",
        "          early_stopping = early_stopping\n",
        "      )\n",
        "      return [self.tokenizer.decode(w, skip_special_tokens=True, clean_up_tokenization_spaces=True) for w in generated_ids]\n",
        "  \n",
        "\n",
        "# Create a dataloading module to hold r/TwoSentenceHorror data as per the PyTorch Lightning Docs\n",
        "class HorrorDataModule(pl.LightningDataModule):\n",
        "  def __init__(self, tokenizer, data_df, batch_size, num_examples = number_top_posts):\n",
        "    super().__init__()\n",
        "    self.tokenizer = tokenizer\n",
        "    self.data_df = data_df\n",
        "    self.batch_size = batch_size\n",
        "    self.num_examples = num_examples\n",
        "  \n",
        "  # Loads and splits the data into training, validation and test sets with a 60/20/20 split\n",
        "  def prepare_data(self):\n",
        "    self.data = self.data_df[:self.num_examples]\n",
        "    self.train, self.validate, self.test = np.split(self.data.sample(frac=1), [int(.6*len(self.data)), int(.8*len(self.data))])\n",
        "\n",
        "  # encode the sentences using the tokenizer  \n",
        "  def setup(self, stage):\n",
        "    self.train = encode_sentences(self.tokenizer, self.train['source'], self.train['target'])\n",
        "    self.validate = encode_sentences(self.tokenizer, self.validate['source'], self.validate['target'])\n",
        "    self.test = encode_sentences(self.tokenizer, self.test['source'], self.test['target'])\n",
        "\n",
        "  # Load the training, validation and test sets in TensorDataset objects\n",
        "  def train_dataloader(self):\n",
        "    dataset = TensorDataset(self.train['input_ids'], self.train['attention_mask'], self.train['labels'])                          \n",
        "    train_data = DataLoader(dataset, sampler = RandomSampler(dataset), batch_size = self.batch_size)\n",
        "    return train_data\n",
        "\n",
        "  def val_dataloader(self):\n",
        "    dataset = TensorDataset(self.validate['input_ids'], self.validate['attention_mask'], self.validate['labels']) \n",
        "    val_data = DataLoader(dataset, batch_size = self.batch_size)                       \n",
        "    return val_data\n",
        "\n",
        "  def test_dataloader(self):\n",
        "    dataset = TensorDataset(self.test['input_ids'], self.test['attention_mask'], self.test['labels']) \n",
        "    test_data = DataLoader(dataset, batch_size = self.batch_size)                   \n",
        "    return test_data\n",
        "\n",
        "# function that shifts input_ids one token to the right, and then wraps last non-pad token (usually <eos>)\n",
        "# taken directly from modeling_bart.py\n",
        "def shift_tokens_right(input_ids, pad_token_id):\n",
        "  prev_output_tokens = input_ids.clone()\n",
        "  index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1) - 1).unsqueeze(-1)\n",
        "  prev_output_tokens[:, 0] = input_ids.gather(1, index_of_eos).squeeze()\n",
        "  prev_output_tokens[:, 1:] = input_ids[:, :-1]\n",
        "  return prev_output_tokens\n",
        "\n",
        "# function that tokenizes a bunch of source sentences for a training dataset\n",
        "# source_sentences and target_sentences correspond to 'source' and 'target' in training data\n",
        "# returns dict with structure {'input_ids':[], 'attention_mask':[], 'target_ids':[]}\n",
        "def encode_sentences(tokenizer, source_sentences, target_sentences, max_length=32, pad_to_max_length=True, return_tensors=\"pt\"):\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "  target_ids = []\n",
        "  tokenized_sentences = {}\n",
        "\n",
        "  # for each source sentence, tokenize and append to input_ids and attention_masks lists\n",
        "  for sentence in source_sentences:\n",
        "    encoded_dict = tokenizer(\n",
        "          sentence,\n",
        "          max_length=max_length,\n",
        "          padding=\"max_length\" if pad_to_max_length else None,\n",
        "          truncation=True,\n",
        "          return_tensors=return_tensors,\n",
        "          add_prefix_space = True\n",
        "      )\n",
        "\n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # do the same for the target sentences, except save in list target_ids\n",
        "  for sentence in target_sentences:\n",
        "    encoded_dict = tokenizer(\n",
        "          sentence,\n",
        "          max_length=max_length,\n",
        "          padding=\"max_length\" if pad_to_max_length else None,\n",
        "          truncation=True,\n",
        "          return_tensors=return_tensors,\n",
        "          add_prefix_space = True\n",
        "      )\n",
        "    target_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "  # flatten the three lists and return batch with all these as a dict\n",
        "  input_ids = torch.cat(input_ids, dim = 0)\n",
        "  attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "  target_ids = torch.cat(target_ids, dim = 0)\n",
        "\n",
        "  return {\"input_ids\": input_ids, \"attention_mask\": attention_masks, \"labels\": target_ids}\n",
        "\n",
        "# Function that noises a sentence by adding random <mask> tokens\n",
        "# sentence_ is the sentence to noise, percent_words is percent of words to replace (rounded up w/ math.ceil)\n",
        "def noise_sentence(sentence_, percent_words, replacement_token = \"<mask>\"):\n",
        "  # Create a list item and copy\n",
        "  sentence_ = sentence_.split(' ')\n",
        "  sentence = sentence_.copy()\n",
        "  \n",
        "  num_words = math.ceil(len(sentence) * percent_words)\n",
        "  \n",
        "  # Create an array of tokens to sample from, can be any word in the sentence\n",
        "  sample_tokens = set(np.arange(0, np.maximum(1, len(sentence))))\n",
        "  words_to_noise = random.sample(sample_tokens, num_words)\n",
        "  \n",
        "  # Swap out words, but not full stops\n",
        "  for pos in words_to_noise:\n",
        "      if sentence[pos] != '.':\n",
        "          sentence[pos] = replacement_token\n",
        "  \n",
        "  # Remove redundant spaces\n",
        "  sentence = re.sub(r' {2,5}', ' ', ' '.join(sentence))\n",
        "  \n",
        "  # Combine concurrent <mask> tokens into a single token; this just does two rounds of this; more could be done\n",
        "  sentence = re.sub(r'<mask> <mask>', \"<mask>\", sentence)\n",
        "  sentence = re.sub(r'<mask> <mask>', \"<mask>\", sentence)\n",
        "  return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pj2f5p-Twa-u"
      },
      "source": [
        "#Load in BART-base and insert data\n",
        "\n",
        "Using BART-base here due to computing constraints. But it still seems to work fairly well! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQr9X97fwenN"
      },
      "source": [
        "# Load the pre-trained model\n",
        "from transformers import BartTokenizer, BartForConditionalGeneration\n",
        "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base', add_prefix_space=True)\n",
        "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWydGvysMpjf"
      },
      "source": [
        "# set up hyperparameters, beam_size is 4\n",
        "hparams = argparse.Namespace()\n",
        "hparams.eval_beams = 4\n",
        "\n",
        "# Load the data into the model for training\n",
        "horror_data = HorrorDataModule(tokenizer, raw_posts, batch_size = 16, num_examples = 200000)\n",
        "model = LitModel(learning_rate = 2e-5, tokenizer = tokenizer, model = bart_model, hparams = hparams)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efCnEGKVHloJ"
      },
      "source": [
        "#Fine-tune BART on r/TwoSentenceHorror data\n",
        "\n",
        "The training time on this was short enough and the space I have on my Google Drive was scarce enough to make me decide to *not* save checkpoints for this model--that means the model does have to train every time, but it's not too expensive time-wise if you run it on a GPU. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319,
          "referenced_widgets": [
            "7fca2f98562141ecb7dbabfad7b12179",
            "fcd5cb589aa648a9a9ffd895d64aaab1",
            "ce70fe14e7e84150832f7b17fe452bac",
            "740dd6f27c474793b7aabadb2bcc60aa",
            "7a346b3d730f4f048611a31c909736e3",
            "4243663306d94fc98f8663ea0c440667",
            "785fa26a95ab40b9a8d581726841e328",
            "245cb62b84924221af208bca2024931f",
            "429b67d94a3645b5a4905e4039e1d605",
            "fb33aa5e5f1e4f648f48160fb705e9e4",
            "ee895919b9164b31bc1a250ca72b841a",
            "c86ca5c4531241deab2cdaf953953efe",
            "246f557df7584fb9a8a7a4ecbd01f4b9",
            "7bc319d997ed4f2b893e4b902a28f564",
            "45e11f5dad9e45b397775b767f7aa23b",
            "1c49a3d2f1704e6f909f3a640f5e46fd",
            "342bb3376e394a4ab068f4f632044d4d",
            "bb2485748e4e4e8e8bf3a4f2de96ad3b",
            "48f23795527c4641af1d4282eec067ac",
            "61b381409f524441ac297797bd4b027b",
            "e4e7efda3f2e4dbdabb2b3576df9128c",
            "7b090cd169f94ffa92ea1ad24172e0e0",
            "588451312e27435abb7bea87ebb48313",
            "bc41218502dc4dee9cde186bd48b852b"
          ]
        },
        "id": "aGKmBYEVY2GZ",
        "outputId": "1ccc84cc-0100-4f34-dff0-db9c25eb1140"
      },
      "source": [
        "trainer = pl.Trainer(gpus = 1,\n",
        "                     max_epochs = 1,\n",
        "                     min_epochs = 1,\n",
        "                     auto_lr_find = True,\n",
        "                     progress_bar_refresh_rate = 500)\n",
        "trainer.fit(model, horror_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 139 M \n",
            "-------------------------------------------------------\n",
            "139 M     Trainable params\n",
            "0         Non-trainable params\n",
            "139 M     Total params\n",
            "557.682   Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7fca2f98562141ecb7dbabfad7b12179",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layoutâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "429b67d94a3645b5a4905e4039e1d605",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), maxâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "342bb3376e394a4ab068f4f632044d4d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), mâ€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhicnbZjn3l4"
      },
      "source": [
        "#Collect first sentences from r/Showerthoughts\n",
        "\n",
        "BART can't come up with ideas out of thin air, so we have to provide it with a first sentence as a prompt for its two-sentence horror story. I found that taking posts from r/Showerthoughts was interesting, because they were about the right length and produced some entertaining results (in my opinion). \n",
        "\n",
        "We can take advantage of the high turnover of r/Showerthoughts by sampling from hot posts. While many posts there are good right out of the box, some consist of two or more sentences, like [this post](https://www.reddit.com/r/Showerthoughts/comments/mpdyxy/dragons_dont_really_breathe_fire_they_just_exhale/?utm_source=share&utm_medium=web2x&context=3):\n",
        "\n",
        "\n",
        "> **Dragons don't really breathe fire, they just exhale it.** That's like saying humans breathe carbon dioxide.\n",
        "\n",
        "Since these sentences are just meant to be jumping-off points for our model, I decided to split the sampled posts on punctuation marks and only feed the **first sentence** to the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3RFLUgiqsND"
      },
      "source": [
        "showerthoughts = []\n",
        "number_showerthoughts = 50 # won't necessarily get exactly this many, because of pinned posts\n",
        "\n",
        "# Sample some showerthoughts from hot posts. only take the first sentence, put a period at the end of it.\n",
        "for submission in reddit.subreddit('showerthoughts').hot(limit=number_showerthoughts):\n",
        "  # Check to make sure it's not a mod-stickied post\n",
        "  if not submission.stickied:\n",
        "    # split into sentences and append the first sentence\n",
        "    submission_sentences = re.split(r'[.!?;:]', submission.title)\n",
        "    first_sentence = re.sub('\\*', '', submission_sentences[0].strip())\n",
        "    showerthoughts.append(first_sentence + '.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T99fVrO0iI39"
      },
      "source": [
        "#Collect first sentences from r/OffMyChest\n",
        "\n",
        "The r/Showerthoughts stories were entertaining, but some of them were pretty wordy, and I found that the novelty of the shower thought itself sometimes distracted from the overall horror of the story. To address this, for the next batch of horror stories, I decided to take the seed sentences from r/OffMyChest. \n",
        "\n",
        "These sentences had a lot higher chances of generating more genuinely scary horror stories, but I wanted to make sure that they were suitably filtered to respect anonymity/privacy of the posters on that subreddit. I decided to:\n",
        "\n",
        "*   remove information about age and gender i.e. \"I ~~(27F)~~ wanted ...\"\n",
        "*   filter out submissions mentioning abuse, sexual assault, eating disorders, suicide, and self-harm \n",
        "\n",
        "I sampled the fourth sentence from hot posts on r/OffMyChest. My reasoning was that earlier sentences would be more general than later ones, which would be too focused on the details of that particular redditor's situation. The reason why I didn't just take the first or second sentence was to avoid some of the waffling that often comes at the beginning of r/OffMyChest posts (i.e. \"this post might be TMI,\" \"using a throwaway account for this,\" \"I changed the names of the people in this post\"). \n",
        "\n",
        "If the sampled sentence was about a sensitive topic, I would just skip over that post entirely. Otherwise, I would look through the sentence and filter out any ages using a couple handmade regexes, which look for cases like `(27F)`, `[M]`, or `19f`. Then, the sentence would be ready to feed into the model. \n",
        "\n",
        "***While actual r/TwoSentenceHorror submissions don't shy away from sensitive topics, using actual people's suffering as prompts for a novelty Twitter bot felt wrong to me (even though it's anonymized). For this reason, I decided to filter sentences more than less. The filtering I implemented is very rudimentary, however, so I also made sure to monitor outputs for sensitve content manually as well.***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrANhsGqnRxJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c194f818-0c6b-4e31-9d21-3271bc27f08a"
      },
      "source": [
        "offmychest = []\n",
        "number_offmychest = 60 # sample a bit more than 50 because expecting some will be filtered out\n",
        "\n",
        "sensitive_words = ['abuse', 'abusive', 'rape', 'rapist', 'assault', 'molest', 'kill', 'suicide', 'self harm', 'cutting', 'self-harm']\n",
        "\n",
        "for submission in reddit.subreddit('offmychest').hot(limit=number_offmychest):\n",
        "  sentences = [s.strip() for s in re.split(r'[.!?;:]', submission.selftext) if s]\n",
        "  if sentences:\n",
        "    fourth_s = sentences[min(3, len(sentences)-1)]\n",
        "    if any(sw in fourth_s for sw in sensitive_words):\n",
        "      print('skipped submission')\n",
        "      continue # skip over this submission if sampled sentence is sensitive\n",
        "    else:\n",
        "      s = re.sub('[\\(\\[][0-9]*[MFmf][\\)\\]]', '', fourth_s) # (17F), (M), (102f], [28m], [f]\n",
        "      s = re.sub(' [0-9][0-9][MmFf]', '', s) # 19f, 20M in the middle of a sentence\n",
        "      s = re.sub(' +', ' ', s) # get rid of extra spaces that result from deletion\n",
        "      s = re.sub('\\*', '', s) # get rid of asterisks\n",
        "      if len(s) <= 140: # if the sentence is not too long\n",
        "        offmychest.append(s + '.')\n",
        "\n",
        "print(offmychest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skipped submission\n",
            "['I made it up there, I could barely see due to smoke and started crawling.', 'Just stop the madness.', 'I have seen grown ass men look at girls as young as 11, and they seem to have no shame.', 'Canâ€™t tell anyone else so I thought Iâ€™d share here.', 'And anti-vaxers latched on and started using whatâ€™s happened as proof of the dangers of vaccines.', \"For the last decade, I've had roadblock after roadblock, both in my personal life and my professional life.\", \"The subreddit rules won't allow me to link it, so just click my profile and few my post.\", \"I don't mind it that much, their mental health is way more important to me.\", \"I'm so happy and proud of myself for pulling through.\", 'I just need to talk about her, cause she makes me so happy.', 'If anyone else also shares my birthday happy birthday.', 'I am still pretty young, and nothing traumatic has happened to me.', ') couldnâ€™t be more happier in myself.', 'Other times I can be watching television, and all of a sudden my arms and legs feel like they are way too long for my body.', 'These teenagers will grow up and be eternally grateful that you didnâ€™t take advantage of their naivety.', \"Once that bundle of cells is born, they don't care anymore.\", 'Long story extremely short, the next fifteen years became a deleterious descent into the insanity that is addiction (to everything).', 'Rings are for fingers and sometimes toes, lady.', 'I applied for a job and two weeks later I signed my offer letter.', \"it's not always easy for a suicidal person to reach out for help, so please check in on your friends.\", 'We fought, fought, and fought until the day came and something hit us hard.', 'Itâ€™s a shame really.', 'And all the time i think that I need a fucking break from this life.', 'ITâ€™S NOT SOMETHING WE CAN CONTROL, ITâ€™S JUST THERE MAN.', '\" or \"Just relax\" whenever I am worked up.', 'recently though iâ€™ve come to a point where i just donâ€™t find anything on it funny anymore.', 'In my country youâ€™re allowed to meet with 1 person (I think, they keep changing regulations I canâ€™t keep upâ‚¬ from your close social circle.', 'My bf is shorted then me by a few centimeters, but usually more because I love wearing platform shoes.', '*fuck periods*.', 'When I went to college, I commuted from home because it was only 20 minutes away and it saved my parents nearly $20,000 a year.', 'I hate myself for it.', 'Ted Talk over.', 'Even as I havenâ€™t stepped into a church in years.', 'Thereâ€™s a husky white bald guy and appears to be the ring leader.', 'Yeah, pretty upset about that, thankful to be working but holy shit this is not what a raise is.', 'i finally got a wake up call from my doctor and finally started my medication again, as well as starting prozac.', 'I donâ€™t want to go back to not wearing masks, Iâ€™m too insecure about how I look now.', \"It's been a particularly tough year and a half for us though as my wife passed away in an accident.\", 'You want to travel to a city which wouldnâ€™t let you enter due to your religion.', \"just a vent*\\n\\n\\\\--\\n\\nIt's difficult to type.\", 'I have friends, and I believe most of these friends actually like me back.', 'I could barely keep still as I was waiting to be screened.', 'Andy replied with \"leave me alone\", \"I\\'ll go to tomorrow\", etc.', 'I canâ€™t tell him this obviously but I totally wouldnâ€™t mind spending the rest of my life with him.', 'What they didnâ€™t test me in was the names of these American English speaking providers.', 'I want to be called a boy, I want to just be a boy.', \"Austin was a drummer, super into Tool, and didn't really vibe with many others in our small rural community.\", 'Oh walking or running.', 'I was 12.', 'I know for a fact I would not be able to cope, and a child that needs such intense and full-on care would deserve a mother who could do it.', 'a variety of reasons I have struggled a lot.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMKOcyliNcFQ"
      },
      "source": [
        "#Write some two-sentence horror stories!\n",
        "\n",
        "`generate_story()` is a function that generates a two-sentence horror story based on a single line as input. I noticed that the stories generated where the beginnings of the two sentences were the same were more boring, so if that happens we try again, this time masking the first token and increasing the noise. I wrote this function so that it keeps regenerating until the first two words of each sentence aren't the same. \n",
        "\n",
        "`generate_stories_json()` is a function that takes in a list of prompts and then generates a horror story for each prompt, dumping the final list of stories into a json file. The json file consists of a list of `{'story':str, 'seen':bool}` dicts, where the `seen=False` field makes it easier to randomly sample and post stories to Twitter later on. If `shorten_for_twitter=True`, the model will keep generating stories until they fit within Twitter's 280 character limit, or just abandon it if it can't get anything short enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7IHYXW1ZmCO"
      },
      "source": [
        "# function that returns a string containing a two-sentence horror story based on a single line as input. \n",
        "# first_sentence is the first half of our horror story, and model_ is the fine-tuned BART model. \n",
        "# I found that noise_percent of 0.3 seemed to generate entertaining results most of the time.\n",
        "def generate_story(first_sentence, model_, noise_percent = 0.3):\n",
        "  # Put the model on eval mode\n",
        "  model_.to(torch.device('cpu'))\n",
        "  model_.eval()\n",
        "\n",
        "  # prep output_story string and noise first_sentence\n",
        "  output_story = ''\n",
        "  output_story += first_sentence\n",
        "  prompt_tokenized = tokenizer(noise_sentence(first_sentence, noise_percent), max_length = 32, return_tensors = \"pt\", truncation = True)\n",
        "\n",
        "  # generate second_sentence from first_sentence. if\n",
        "  second_sentence = model.generate_text(prompt_tokenized, eval_beams = 5)[0].strip()\n",
        "\n",
        "  # if the first word of punchline is the same as first word of setup, keep trying \n",
        "  noise_boost = 0\n",
        "  while second_sentence.split(' ')[0] == first_sentence.split(' ')[0]:\n",
        "    np_first_sentence = np.array(first_sentence.split(' '))\n",
        "    np_first_sentence[0] = '<mask>'\n",
        "    first_sentence_masked = ' '.join(np_first_sentence)\n",
        "    prompt_tokenized = tokenizer(noise_sentence(first_sentence_masked, noise_percent+noise_boost), max_length = 32, return_tensors = \"pt\", truncation = True)\n",
        "    second_sentence = model.generate_text(prompt_tokenized, eval_beams = 5)[0].strip()\n",
        "    noise_boost += 0.02\n",
        "\n",
        "  # append to output and return it\n",
        "  output_story += '\\n\\n' \n",
        "  output_story += second_sentence\n",
        "  return output_story\n",
        "\n",
        "# function that takes in a list of first sentences and generates a two-sentence horror story for each one\n",
        "# returns a list of finalized stories as strings\n",
        "def generate_stories(prompt_list, shorten_for_twitter=False):\n",
        "  stories = []\n",
        "  for prompt in prompt_list:\n",
        "    story = generate_story(prompt, model)\n",
        "\n",
        "    if shorten_for_twitter: # try to keep it under 280 characters\n",
        "      safety_counter = 5 # try 5 times before giving up\n",
        "      while len(story) > 280 and safety_counter > 0:\n",
        "        story = generate_story(prompt, model)\n",
        "        safety_counter -= 1\n",
        "      if len(story) <= 280:\n",
        "        stories.append(story)\n",
        "\n",
        "    else: \n",
        "      stories.append(story)\n",
        "  \n",
        "  return stories\n",
        "\n",
        "# function that calls generate_stories(), and then prints and saves them\n",
        "def print_and_json(list_of_prompts):\n",
        "  # generate stories for the whole list using function above\n",
        "  stories = generate_stories(list_of_prompts, shorten_for_twitter = True)\n",
        "  \n",
        "  # print results\n",
        "  for story in stories:\n",
        "    print(story + '\\n\\n')\n",
        "\n",
        "  # Create dictionary and then dump it into json\n",
        "  story_json_list = [{'story': s, 'seen': False} for s in stories]\n",
        "  with open(datetime.now().strftime(\"stories_%d_%m_%H%M.json\"), 'w') as f:\n",
        "    json.dump(story_json_list, f)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hJ3ptIQiRV8"
      },
      "source": [
        "# print_and_json(showerthoughts)\n",
        "print_and_json(offmychest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZydV0xwRoSX",
        "outputId": "f69322eb-d18c-46da-afe5-ba4a6f83dd82"
      },
      "source": [
        "# Line of code to generate a single story from custom prompt\n",
        "print(generate_story('I was working in the lab late one night.', model))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I was working in the lab late one night.\n",
            "\n",
            "But I was working in the lab all night.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}